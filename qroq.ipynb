{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain_community beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting docx\n",
      "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 161 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting lxml\n",
      "  Downloading lxml-5.3.0-cp39-cp39-macosx_10_9_universal2.whl (8.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.1 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow>=2.0 in /Users/shivamagharkar/Library/Python/3.9/lib/python/site-packages (from docx) (10.1.0)\n",
      "Building wheels for collected packages: docx\n",
      "  Building wheel for docx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53924 sha256=96a9655a8c8e7444f36be6336db912165df51ec07609bc23b3f9dce8d9f3a5d2\n",
      "  Stored in directory: /Users/shivamagharkar/Library/Caches/pip/wheels/0f/8e/9d/7003eed35a84cf960876aae6bdf60d02041ddfcca66eceee94\n",
      "Successfully built docx\n",
      "Installing collected packages: lxml, docx\n",
      "Successfully installed docx-0.2.4 lxml-5.3.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install docx\n",
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.17.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting tensorflow<2.18,>=2.17 (from tf-keras)\n",
      "  Downloading tensorflow-2.17.0-cp311-cp311-macosx_12_0_arm64.whl (236.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.2/236.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.6.3)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow<2.18,>=2.17->tf-keras)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (4.24.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (67.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (4.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.60.0)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow<2.18,>=2.17->tf-keras)\n",
      "  Downloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras>=3.2.0 (from tensorflow<2.18,>=2.17->tf-keras)\n",
      "  Downloading keras-3.5.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.36.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.26.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf-keras) (0.38.4)\n",
      "Requirement already satisfied: rich in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (13.6.0)\n",
      "Requirement already satisfied: namex in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.0.7)\n",
      "Requirement already satisfied: optree in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/shivamagharkar/miniconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.1.2)\n",
      "Installing collected packages: flatbuffers, tensorboard, keras, tensorflow, tf-keras\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 24.3.7\n",
      "    Uninstalling flatbuffers-24.3.7:\n",
      "      Successfully uninstalled flatbuffers-24.3.7\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.16.2\n",
      "    Uninstalling tensorboard-2.16.2:\n",
      "      Successfully uninstalled tensorboard-2.16.2\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.1.1\n",
      "    Uninstalling keras-3.1.1:\n",
      "      Successfully uninstalled keras-3.1.1\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.16.1\n",
      "    Uninstalling tensorflow-2.16.1:\n",
      "      Successfully uninstalled tensorflow-2.16.1\n",
      "Successfully installed flatbuffers-24.3.25 keras-3.5.0 tensorboard-2.17.1 tensorflow-2.17.0 tf-keras-2.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivamagharkar/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting PDF data: module 'fitz' has no attribute 'open'\n",
      "Resume Data in DataFrame:\n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "Raw LLM response:\n",
      " ```json\n",
      "{\n",
      "  \"role\": \"ML & Data Science Engineer\",\n",
      "  \"experience\": \"Not specified\",\n",
      "  \"skills\": [\n",
      "    \"Machine Learning\",\n",
      "    \"Data Science\",\n",
      "    \"Python\",\n",
      "    \"Pytorch\",\n",
      "    \"Scipy\",\n",
      "    \"XGBoost\",\n",
      "    \"Snowflake\",\n",
      "    \"NLP\",\n",
      "    \"Tabular modeling\",\n",
      "    \"Forecasting\",\n",
      "    \"Normalization\"\n",
      "  ],\n",
      "  \"description\": \"We are looking for a builder who is fearless in challenging conventions and bringing new ideas to the table. As an ML & Data Science Engineer, you will own the machine room of the data products driving our business, improve our automated campaign steering, and advance our targeting by extracting every ounce of entertainment intelligence from our recommendation system based on massive user and item data.\"\n",
      "}\n",
      "```\n",
      "Extracted Job Data: {'role': 'ML & Data Science Engineer', 'experience': 'Not specified', 'skills': ['Machine Learning', 'Data Science', 'Python', 'Pytorch', 'Scipy', 'XGBoost', 'Snowflake', 'NLP', 'Tabular modeling', 'Forecasting', 'Normalization'], 'description': 'We are looking for a builder who is fearless in challenging conventions and bringing new ideas to the table. As an ML & Data Science Engineer, you will own the machine room of the data products driving our business, improve our automated campaign steering, and advance our targeting by extracting every ounce of entertainment intelligence from our recommendation system based on massive user and item data.'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'skills'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 133\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob data could not be extracted. Please check the response format.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Extract and compare skills from resume and job description\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     student_skills \u001b[38;5;241m=\u001b[39m \u001b[43mresume_json_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskills\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    134\u001b[0m     job_skills \u001b[38;5;241m=\u001b[39m job_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskills\u001b[39m\u001b[38;5;124m'\u001b[39m, [])\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# Convert skills into embeddings using SentenceTransformer\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/core/indexes/range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'skills'"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import json\n",
    "from io import StringIO\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load pre-trained language model\n",
    "nlp = spacy.load(\"en_core_web_md\")  # For semantic similarity\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')  # For sentence embeddings\n",
    "\n",
    "# Extract PDF data into a DataFrame\n",
    "def extract_pdf_data(pdf_path):\n",
    "    skills = []\n",
    "    experience = []\n",
    "    education = []\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)  # Open the PDF document\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)  # Load each page\n",
    "            text = page.get_text()\n",
    "            lines = text.split('\\n')\n",
    "            \n",
    "            is_skills_section = is_experience_section = is_education_section = False\n",
    "            \n",
    "            for line in lines:\n",
    "                # Handle section starts\n",
    "                if 'Skills' in line or 'Technical Skills' in line:\n",
    "                    is_skills_section = True\n",
    "                    is_experience_section = is_education_section = False\n",
    "                    continue\n",
    "                elif 'Experience' in line:\n",
    "                    is_experience_section = True\n",
    "                    is_skills_section = is_education_section = False\n",
    "                    continue\n",
    "                elif 'Education' in line:\n",
    "                    is_education_section = True\n",
    "                    is_skills_section = is_experience_section = False\n",
    "                    continue\n",
    "                \n",
    "                # Extract data within respective sections\n",
    "                if is_skills_section and line.strip():\n",
    "                    skills.extend([skill.strip() for skill in line.split(',') if skill.strip()])\n",
    "                elif is_experience_section and line.strip():\n",
    "                    experience.append(line.strip())\n",
    "                elif is_education_section and line.strip():\n",
    "                    education.append(line.strip())\n",
    "        \n",
    "        doc.close()  # Close the document\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        data = {\n",
    "            \"skills\": skills if skills else [\"No skills found\"],\n",
    "            \"experience\": experience if experience else [\"No experience found\"],\n",
    "            \"education\": education if education else [\"No education found\"]\n",
    "        }\n",
    "        return pd.DataFrame([data])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting PDF data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load resume and extract data\n",
    "pdf_resume_path = \"/Users/shivamagharkar/Desktop/Resumes/SHIVAM_AGHARKAR_-_CV.pdf\"  # Update this path\n",
    "resume_df = extract_pdf_data(pdf_resume_path)\n",
    "print(\"Resume Data in DataFrame:\\n\", resume_df)\n",
    "\n",
    "# Convert DataFrame to JSON\n",
    "resume_json = resume_df.to_json(orient=\"records\")\n",
    "resume_json_dict = pd.read_json(StringIO(resume_json))\n",
    "\n",
    "# Scrape the job data using WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://jobs.lever.co/justwatch/27468019-6219-4007-a69f-0453441c96f4?lever-source=LinkedIn&source=6\")\n",
    "page_data = loader.load().pop().page_content\n",
    "\n",
    "# LLM setup for job extraction\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    groq_api_key='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',  # Replace with your actual API key\n",
    "    model_name=\"llama-3.1-70b-versatile\"\n",
    ")\n",
    "\n",
    "# Define a job extraction prompt\n",
    "prompt_extract = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    ### SCRAPED TEXT FROM WEBSITE:\n",
    "    {page_data}\n",
    "    ### INSTRUCTION:\n",
    "    Extract job postings from the website and return them in JSON format with the following keys: \n",
    "    'role', 'experience', 'skills', and 'description'.\n",
    "    Return only valid JSON.\n",
    "    ### VALID JSON (NO PREAMBLE):\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Extract job data using LLM with error handling\n",
    "chain_extract = prompt_extract | llm\n",
    "res = chain_extract.invoke(input={'page_data': page_data})\n",
    "\n",
    "# Print raw response for debugging\n",
    "print(\"Raw LLM response:\\n\", res.content)\n",
    "\n",
    "# Initialize job_data\n",
    "job_data = None\n",
    "\n",
    "# Check if the LLM returned valid JSON\n",
    "try:\n",
    "    if isinstance(res.content, bytes):\n",
    "        response_content = res.content.decode('utf-8')  # Convert bytes to string\n",
    "    else:\n",
    "        response_content = res.content\n",
    "\n",
    "    response_content = response_content.strip('```json').strip('```')\n",
    "\n",
    "    job_data = json.loads(response_content)\n",
    "    print(\"Extracted Job Data:\", job_data)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(\"JSONDecodeError occurred:\", e)\n",
    "    print(\"Raw response content is not valid JSON:\\n\", response_content)\n",
    "except Exception as e:\n",
    "    print(\"An unexpected error occurred:\", e)\n",
    "    print(\"Raw response content:\\n\", response_content)\n",
    "\n",
    "# Check if job_data was successfully defined\n",
    "if job_data is None:\n",
    "    print(\"Job data could not be extracted. Please check the response format.\")\n",
    "else:\n",
    "    # Extract and compare skills from resume and job description\n",
    "    student_skills = resume_json_dict['skills'].iloc[0]\n",
    "    job_skills = job_data.get('skills', [])\n",
    "\n",
    "    # Convert skills into embeddings using SentenceTransformer\n",
    "    student_skill_embeddings = model.encode(student_skills)\n",
    "    job_skill_embeddings = model.encode(job_skills)\n",
    "\n",
    "    # Find the cosine similarity between each skill from resume and job description\n",
    "    similarities = util.cos_sim(student_skill_embeddings, job_skill_embeddings)\n",
    "\n",
    "    # Set a threshold for considering skills as matched\n",
    "    similarity_threshold = 0.7\n",
    "\n",
    "    # Extract matches based on the similarity threshold\n",
    "    matched_skills = set()\n",
    "    for i, student_skill in enumerate(student_skills):\n",
    "        for j, job_skill in enumerate(job_skills):\n",
    "            similarity_score = similarities[i][j]\n",
    "            if similarity_score > similarity_threshold:\n",
    "                matched_skills.add(job_skill)  # Or add both if you want exact pairs\n",
    "\n",
    "    print(f\"Matched Skills: {matched_skills}\")\n",
    "\n",
    "    # Define a prompt for cold email generation\n",
    "    prompt_email = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        ### JOB DESCRIPTION:\n",
    "        {job_description}\n",
    "        \n",
    "        ### INSTRUCTION:\n",
    "        Write a cold email mentioning the matched skills: {matched_skills}.\n",
    "        \n",
    "        ### EMAIL (NO PREAMBLE):\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Generate the cold email using LLM\n",
    "    chain_email = prompt_email | llm\n",
    "    email_response = chain_email.invoke({\n",
    "        \"job_description\": str(job_data),\n",
    "        \"matched_skills\": ', '.join(matched_skills)\n",
    "    })\n",
    "\n",
    "    # Print the generated cold email\n",
    "    print(\"Generated Cold Email:\\n\", email_response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import json\n",
    "from io import StringIO\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load pre-trained language model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')  # For sentence embeddings\n",
    "\n",
    "# Function to extract PDF data into a DataFrame\n",
    "def extract_pdf_data(pdf_path):\n",
    "    skills = []\n",
    "    experience = []\n",
    "    education = []\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)  # Open the PDF document\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)  # Load each page\n",
    "            text = page.get_text()\n",
    "            lines = text.split('\\n')\n",
    "            \n",
    "            is_skills_section = is_experience_section = is_education_section = False\n",
    "            \n",
    "            for line in lines:\n",
    "                # Handle section starts\n",
    "                if 'Skills' in line or 'Technical Skills' in line:\n",
    "                    is_skills_section = True\n",
    "                    is_experience_section = is_education_section = False\n",
    "                    continue\n",
    "                elif 'Experience' in line:\n",
    "                    is_experience_section = True\n",
    "                    is_skills_section = is_education_section = False\n",
    "                    continue\n",
    "                elif 'Education' in line:\n",
    "                    is_education_section = True\n",
    "                    is_skills_section = is_experience_section = False\n",
    "                    continue\n",
    "                \n",
    "                # Extract data within respective sections\n",
    "                if is_skills_section and line.strip():\n",
    "                    skills.extend([skill.strip() for skill in line.split(',') if skill.strip()])\n",
    "                elif is_experience_section and line.strip():\n",
    "                    experience.append(line.strip())\n",
    "                elif is_education_section and line.strip():\n",
    "                    education.append(line.strip())\n",
    "        \n",
    "        doc.close()  # Close the document\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        data = {\n",
    "            \"skills\": skills if skills else [\"No skills found\"],\n",
    "            \"experience\": experience if experience else [\"No experience found\"],\n",
    "            \"education\": education if education else [\"No education found\"]\n",
    "        }\n",
    "        return pd.DataFrame([data])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting PDF data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load resume and extract data\n",
    "pdf_resume_path = \"/content/SHIVAM_AGHARKAR_-_CV.pdf\"  # Update with actual path\n",
    "resume_df = extract_pdf_data(pdf_resume_path)\n",
    "student_skills = [skill.lower().strip() for skill in resume_df['skills'].iloc[0]]\n",
    "\n",
    "# Load job data using WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://jobs.lever.co/justwatch/27468019-6219-4007-a69f-0453441c96f4?lever-source=LinkedIn&source=6\")  # Update with actual URL\n",
    "page_data = loader.load().pop().page_content\n",
    "\n",
    "# LLM setup for job extraction\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    groq_api_key='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',  # Replace with your actual API key\n",
    "    model_name=\"llama-3.1-70b-versatile\"\n",
    ")\n",
    "\n",
    "# Define a job extraction prompt\n",
    "prompt_extract = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    ### SCRAPED TEXT FROM WEBSITE:\n",
    "    {page_data}\n",
    "    ### INSTRUCTION:\n",
    "    Extract skills from the job posting.\n",
    "    Return skills in JSON format.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Extract job data\n",
    "chain_extract = prompt_extract | llm\n",
    "res = chain_extract.invoke(input={'page_data': page_data})\n",
    "\n",
    "# Check if response is valid JSON\n",
    "try:\n",
    "    print(\"Raw LLM Response:\", res.content)  # Debug: Print the raw response\n",
    "    job_data = json.loads(res.content) if res.content else {}\n",
    "except json.JSONDecodeError as e:\n",
    "    print(\"JSONDecodeError:\", e)\n",
    "    print(\"Raw response content is not valid JSON:\\n\", res.content)\n",
    "    job_data = {}\n",
    "\n",
    "# Extract job skills\n",
    "job_skills = [skill.lower().strip() for skill in job_data.get('skills', [])]\n",
    "\n",
    "# Print extracted skills for debugging\n",
    "print(\"Student Skills:\", student_skills)\n",
    "print(\"Job Skills:\", job_skills)\n",
    "\n",
    "# Check if job_skills is empty before proceeding\n",
    "if not job_skills:\n",
    "    print(\"No job skills extracted. Cannot compute similarity.\")\n",
    "else:\n",
    "    # Compute embeddings and similarities\n",
    "    student_skill_embeddings = model.encode(student_skills)\n",
    "    job_skill_embeddings = model.encode(job_skills)\n",
    "    \n",
    "    # Ensure both embeddings are not empty before computing similarities\n",
    "    if student_skill_embeddings.size == 0 or job_skill_embeddings.size == 0:\n",
    "        print(\"One of the skill sets has no embeddings. Cannot compute similarity.\")\n",
    "    else:\n",
    "        similarities = util.cos_sim(student_skill_embeddings, job_skill_embeddings)\n",
    "\n",
    "        # Set a threshold for considering skills as matched\n",
    "        similarity_threshold = 0.10\n",
    "\n",
    "        # Extract matches based on the similarity threshold\n",
    "        matched_skills = set()\n",
    "        for i, student_skill in enumerate(student_skills):\n",
    "            for j, job_skill in enumerate(job_skills):\n",
    "                similarity_score = similarities[i][j]\n",
    "                print(f\"Similarity between '{student_skill}' and '{job_skill}': {similarity_score:.4f}\")\n",
    "                if similarity_score > similarity_threshold:\n",
    "                    matched_skills.add(job_skill)\n",
    "\n",
    "        print(f\"Matched Skills: {matched_skills}\")\n",
    "\n",
    "# Generate a cold email if there are matched skills\n",
    "if matched_skills:\n",
    "    prompt_email = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        ### JOB DESCRIPTION:\n",
    "        {job_description}\n",
    "        \n",
    "        ### INSTRUCTION:\n",
    "        Write a cold email mentioning the matched skills: {matched_skills}.\n",
    "        \n",
    "        ### EMAIL (NO PREAMBLE):\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Generate the cold email using LLM\n",
    "    chain_email = prompt_email | llm\n",
    "    email_response = chain_email.invoke({\n",
    "        \"job_description\": str(job_data),\n",
    "        \"matched_skills\": ', '.join(matched_skills)\n",
    "    })\n",
    "\n",
    "    # Print the generated cold email\n",
    "    print(\"Generated Cold Email:\\n\", email_response.content)\n",
    "else:\n",
    "    print(\"No matched skills found. Please check the extraction and comparison process.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
